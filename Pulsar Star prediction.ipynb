{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9a9a06-f4b8-4d38-869a-26131d099629",
   "metadata": {},
   "source": [
    "# Pulsar Star Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cd34e-f9d6-47c4-91ff-0a11e56a0795",
   "metadata": {},
   "source": [
    "Notebook is in github\n",
    "https://github.com/alhu1054/Pulsar_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71614c6a-792d-45e8-9d65-29e15162a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "# Set color map to have light blue background\n",
    "sns.set()\n",
    "import altair as alt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09be942-a200-4765-9fcf-80532b9929b9",
   "metadata": {},
   "source": [
    "Pulsar Stars are neutron stars that look like flickering stars from earth as they radiate two beams of steady light in opossite directions, but because they are rotating the light that gets to earth looks flickering. They are fascinating by itselves but they also have plenty of applications. They have been used to test the general relativity theory under extreme gravitational conditions, as objects to create spacial maps, to create prescise clocks, probes of interstellar medium, probes of space time and to search for gravitational waves among other things. So identifying posible pulsar stars can be of great help to astronomers and scientitis for use in such applications. \n",
    "https://en.wikipedia.org/wiki/Pulsar\n",
    "https://www.space.com/32661-pulsars.html\n",
    "\n",
    "\n",
    "The data of pulsar candidates comes from Kaggel and was collected during the High Time Resolution Universe Survey\n",
    "https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate\n",
    "It contains statistics of the collected data for each object in the universe survey with some preprocesed data to be in kaggle.  It is clearly labeled for a machine learning classification problem, which is what I will do in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9cdf69-f751-4ab1-88e0-08cf6928d3ee",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Data Cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24a7e5-cc01-4b0f-9ea9-e239c015b407",
   "metadata": {},
   "source": [
    "First we load the data from the CSV file and change the names of the columns make it easier to deal with them. The cvs file was downloaded from 'https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate/download?datasetVersionNumber=1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299b4f23-19e7-4b9e-8513-4989ac99b62c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Final_Project/pulsar_data_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n5/yyv4zxgx3xb4rdrwt70vgb380000gn/T/ipykernel_97800/1426714840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpulsar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final_Project/pulsar_data_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpulsar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Final_Project/pulsar_data_train.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "pulsar = pd.read_csv('Final_Project/pulsar_data_train.csv')\n",
    "pulsar.columns = pulsar.columns.str.lstrip()\n",
    "pulsar.columns = pulsar.columns.str.replace(' ','_')\n",
    "pulsar.columns = pulsar.columns.str.replace('-','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9723db3-64dc-41a1-871a-3aa1036c92f2",
   "metadata": {},
   "source": [
    "Let's start the data analisys by looking at the histogram of each feature in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b626d72-60cc-48d0-b7bc-89e81666ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,12))\n",
    "ax = fig.gca()\n",
    "pulsar.hist(ax = ax, bins =100);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef850784-0122-451e-8e38-f288a3bd8bce",
   "metadata": {},
   "source": [
    "From the histograms we can see there are different scales in the features, so we will probaly need to scale or regularize the features. In the target class histogram we see that the data is imbalanced. Some of the histograms are heavealy charged to the left, this makes me suspect of outliers. So i will check for them in scatter plots where the color is the target class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfdd94-db0c-40ea-9447-872853c7d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pulsar.columns.values[0:8]\n",
    "x = np.arange(0,len(pulsar))\n",
    "fig, axs = plt.subplots(4,2 ,sharex=True,figsize = (20,20))\n",
    "axs[0, 0].scatter(x,pulsar[c[0]],c = pulsar.target_class ,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[0, 0].set_title(c[0])\n",
    "axs[0, 1].scatter(x,pulsar[c[1]],c = pulsar.target_class,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[0, 1].set_title(c[1])\n",
    "axs[1, 0].scatter(x,pulsar[c[2]],c = pulsar.target_class,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[1, 0].set_title(c[2])\n",
    "axs[1, 1].scatter(x,pulsar[c[3]],c = pulsar.target_class,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[1, 1].set_title(c[3])\n",
    "axs[2, 0].scatter(x,pulsar[c[4]],c = pulsar.target_class ,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[2, 0].set_title(c[4])\n",
    "axs[2, 1].scatter(x,pulsar[c[5]],c = pulsar.target_class,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[2, 1].set_title(c[5])\n",
    "axs[3, 0].scatter(x,pulsar[c[6]],c = pulsar.target_class ,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[3, 0].set_title(c[6])\n",
    "axs[3, 1].scatter(x,pulsar[c[7]],c = pulsar.target_class,s=3,cmap = 'bwr',alpha = 0.8)\n",
    "axs[3, 1].set_title(c[7])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d5944-3c4a-4af0-a49e-c667c270e931",
   "metadata": {},
   "source": [
    "From the scatter plots we see there aren't really outliers but the data is sparse for some large values of some of the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e5da6-4929-4b04-985c-445f1e6235bd",
   "metadata": {},
   "source": [
    "Because the scales of each feature are very different I will scale all the data using min-max scaling so every feature is between 0 and 1 and when I use methods like regression the data will be nicely scaled. This kind of scaling doesnt change the correlation between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1757a-3778-427c-a596-e2da4cf8e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "norm = MinMaxScaler()\n",
    "p_norm = norm.fit_transform(pulsar)\n",
    "p_norm = pd.DataFrame(p_norm, columns = pulsar.columns)\n",
    "p_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b40c9-d122-4791-a96e-74de0c5b5a6d",
   "metadata": {},
   "source": [
    "Now let's look at the information of the data, to check exactly how many instances we have and how many null values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3862dd-1709-4778-ab89-0685028551f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulsar.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412d362-1564-45f3-af82-3b168c1a6173",
   "metadata": {},
   "source": [
    "We see that the data has 8 features and a target indicating if it is a pulsar or not. \n",
    "The problem to solve is a classification problem where we use some or all the features to predict if the light used to get the features comes from a pulsar or not. \n",
    "\n",
    "The data has a total of 12528 instances, but some of the features have null data, such as Excess_kurtosis_of_the_integrated_profile that has almost the 14% of its rows as null, the Standard_deviation_of_the_DM_SNR_curve has around 9% of null values, and the Skewness_of_the_DM_SNR_curve has 5% of it's values null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5505d-1b41-4524-bf5d-c513e5601c2b",
   "metadata": {},
   "source": [
    "To decide what to do with the null values we take a look at both the correlation between al the features, including if the data comes from a pulsar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f000c-b3de-4529-b0f9-880470ba7607",
   "metadata": {},
   "source": [
    "### Correlation between the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf0e07-2948-47eb-b85e-4b8e71f6fa2d",
   "metadata": {},
   "source": [
    "The correlation matrix between the features can tell use what will be the most important feature for the classification as well as posible correlations to be aware of between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cee842-ff9c-450e-b6f5-107d09f66a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 7.4))\n",
    "sns.set(font_scale=1.6)\n",
    "sns.heatmap(p_norm.corr(),annot=True,fmt = \".2f\", center = 0, vmin = -1, vmax = 1,linewidths=.5, ax = ax,square = True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6969b-4016-463a-b80e-58c93147d234",
   "metadata": {},
   "source": [
    "From the correlation plot we learn that the highest correlation between the target class and the features is with Excess kurtosis of the integrated profile at 0.79, which is also the feature with the most missing values. So we need to be carefull on how to deal with such values. To have a bot more information about how the target class relates with the rest of the features and how do they related with each other we look at the pair plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873367eb-aa5e-46d0-8b95-e526e3f23c9c",
   "metadata": {},
   "source": [
    "### The pair plot of each feature with the color of each data point representing the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d91db8-6ee0-42e7-a8ce-c5be6d7a6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(p_norm,diag_kind = 'kde', hue = 'target_class')\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    # rotate x axis labels\n",
    "    ax.set_xlabel(ax.get_xlabel(), rotation = 90)\n",
    "    # rotate y axis labels\n",
    "    ax.set_ylabel(ax.get_ylabel(), rotation = 0)\n",
    "    # set y labels alignment\n",
    "    ax.yaxis.get_label().set_horizontalalignment('right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399e4a6-12be-481a-84a8-bb4900b6982a",
   "metadata": {},
   "source": [
    "In the pair plot we can observe that the target class orange if =1 means its a pulsar and blue = 0 means is not, looks easier to separate in the plots by orange and blue involving the Integrated profile over the ones of the DM SNR curve. So I expect the resulting model to depend more on first 4 features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f0622-1a8f-4c5c-8b19-9d682c6494ca",
   "metadata": {},
   "source": [
    "By rule of thumb from how many missing values each feature has,we should get rid of the Excess_kurtosis_of_the_integrated_profile as it has more than 10% of missing values, however because it is the one with the highest correlation with the target class, I will try to input the missing values by trying to see if I can fit that feature using multilinear regresion with the rest of the features from the integrated profile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2e6de-415f-4f52-b397-6c5e5e50a5d4",
   "metadata": {},
   "source": [
    "From both the correlation and the pair plot we see that the excess kurtosis of the integrated profile is highly correlated with the skewness and the mean of the integrated profile if we look at the plot of Skweness vs Excess Kurtosis we can see the shape it's close to the square root. Let's look at the plot by itself with the color of the dots representing the standard deviation of the integrated profile to see if it's adds further information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b192707-3c04-4d96-a720-563a6636a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "ax.scatter(p_norm.Skewness_of_the_integrated_profile,p_norm.Excess_kurtosis_of_the_integrated_profile,\n",
    "           c = p_norm.Standard_deviation_of_the_integrated_profile, s =5)\n",
    "x = np.arange(0,1,0.01)\n",
    "ax.plot(x,np.power(0.7*x,0.55)+0.2,label = \"0.2+(0.7X)^0.55\",c = 'g')\n",
    "ax.set_xlabel('Skewness_of_the_integrated_profile')\n",
    "ax.set_ylabel('Excess_kurtosis_of_the_integrated_profile')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e7428-6f98-435d-b48d-91d9406e423b",
   "metadata": {},
   "source": [
    "In the plot above the color is the standar deviation of the integrated profile and we can see it does add information to the plot. The green line than envolves the data is the (0.7*Skeness)^0.55+0.2 and I got to it with trial and error by eye, so if we combine the Skewness^0.55 with the Santandar deviation and the Mean in a linear regression model we might get a good aproximation of the excess kurtosis to substitute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b50c2-43a5-47e7-8a60-5a66b9e033ee",
   "metadata": {},
   "source": [
    "### Linear Regression to get the missing Excess Kurtosis of the integrated profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c6070-26bb-4319-a4c3-672c90d728fb",
   "metadata": {},
   "source": [
    "Using the data for the integrated profile and multilinear regression I fit a model to get the missing data points for the Excess Kurtosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fd9c0-7f3b-45ff-8d81-5218b93c142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test= train_test_split(p_norm,test_size=0.15, random_state= 47)\n",
    "\n",
    "model = \\\n",
    "smf.ols(formula=\\\n",
    "        'Excess_kurtosis_of_the_integrated_profile~np.power(Skewness_of_the_integrated_profile,0.55)\\\n",
    "         +Standard_deviation_of_the_integrated_profile+Mean_of_the_integrated_profile', \n",
    "                data=X_train).fit()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b16abb-5af2-4867-a1ce-bbf68e0f1970",
   "metadata": {},
   "source": [
    "As we can see from the model summary the adjusted R^2 has a pretty high value of 0.971 indicating that it is a good prediction and all the p_values for the coeficients are below 0.001. To get the adjusted R_Squared of the test data I use the function I wrote to get it in module 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4d4e5-faa0-4342-b058-431cb61b358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_RS(model, data,dep_var):\n",
    "    k = len(model.params)-1\n",
    "    n = len(data[dep_var])\n",
    "    dm = model.predict(data)\n",
    "    RSS = sum((data[dep_var]-dm)**2)\n",
    "    den_m = data[dep_var].mean()\n",
    "    TSS = sum((data[dep_var]-den_m)**2)\n",
    "    RS= 1-(RSS/TSS)  \n",
    "    Adj_RS = 1-(((1-RS)*(n-1))/(n-k-1))\n",
    "    return(Adj_RS)\n",
    "test_R2a = adj_RS(model,X_test[X_test.Excess_kurtosis_of_the_integrated_profile.isnull()==False],\n",
    "                  'Excess_kurtosis_of_the_integrated_profile')\n",
    "print('The adjusted R square of the test data is ' + str(np.round(test_R2a,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dcbfe-28a4-4e5b-807b-b33dd3429969",
   "metadata": {},
   "source": [
    "We got a pretty good R square value for the test data too. If we plot the results of the prediction against the actual Excess Kurtosis values we get the next plot. With that target class as a color. Blue dots correspond to target class 0 while red dots corresponds to 1. A perfect prediction would be a line of slope 1 like the green line on the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a94c0-564d-40ca-86b0-48599c700b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "ax.scatter(p_norm.Excess_kurtosis_of_the_integrated_profile,model.predict(p_norm),\n",
    "           c = p_norm.target_class,s =3,alpha=0.3,cmap = 'seismic')\n",
    "ax.plot(np.arange(0,1,.001),np.arange(0,1,.001),c = 'g')\n",
    "ax.set_xlabel('Data Excess_kurtosis_of_the_integrated_profile')\n",
    "ax.set_ylabel('Predicted Excess_kurtosis_of_the_integrated_profile')\n",
    "plt.savefig('pred_plot.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0def7-c038-4729-afa1-0f008cc59f9f",
   "metadata": {},
   "source": [
    "We can see that from 0.2 to 1 the prediction is pretty good, but it brokes down for values betwenn 0 and 0.2 as expected. However it seems that for small Excess kurtosis of the integrated profile, the sign doesn't change the prediction of the result value so I will imput the missing values with the model aproximation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295478a-aaae-4798-820c-833f4c07008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_norm.loc[p_norm['Excess_kurtosis_of_the_integrated_profile'].isnull(),\n",
    "           'Excess_kurtosis_of_the_integrated_profile'] = \\\n",
    "model.predict(p_norm[p_norm.Excess_kurtosis_of_the_integrated_profile.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898677c6-4b48-4156-b11b-c00fc98ee896",
   "metadata": {},
   "source": [
    "From the correlation matrix and the pair plot we see that the other 2 features with missing values don't seem to be that important to predict the target class, so we will drop the Standard_deviation_of_the_DM_SNR_curve and the missing rows of the Skewness_of_the_DM_SNR_curve that has only 5% missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6027a96-98f8-44d0-944b-786a8e232bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_norm = p_norm.drop(columns = ['Standard_deviation_of_the_DM_SNR_curve'])\n",
    "p_norm = p_norm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc1783-8cc9-45b8-962c-8f29ba010a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_norm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cffb2-64d4-490d-9022-13559b10766b",
   "metadata": {},
   "source": [
    "As the final step of the exploratory data analis and data clean up lets see how imbalanced is the target class, with the mean value of the target class. A perfectly balnced target class would have 0.5 as a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c3e3d-0857-4dd5-af93-6fe4c3f47e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Porcentage of pulsars in data '+str(np.round(100*np.mean(pulsar.target_class),0))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997115-e9f8-4b11-916d-4bff28bea4da",
   "metadata": {},
   "source": [
    "This means tha the data is a bit imbalanced as more than 90% of it is from not pulsar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ed7fb-ba26-4f61-954a-e0f84f6e0c7d",
   "metadata": {},
   "source": [
    "## Machine Learning Classification Problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb307afe-22da-440a-9b9f-456a6ce94e6f",
   "metadata": {},
   "source": [
    "The data is now ready to solve the classification problem that will determine if a star its a pulsar or not. I will try different models and see which one works best. \n",
    "First thing to do is separate the data in train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba016f47-fead-42e6-96ce-080c410bd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train, y_test = train_test_split(p_norm.loc[:, p_norm.columns != 'target_class'],\n",
    "                                                   p_norm.target_class, test_size=0.30, random_state=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa141f-485b-497b-9d80-4da114c355f4",
   "metadata": {},
   "source": [
    "And use the functions from module 4 to evaluate the precision and recall and F1, because the data is imbalanced, we wont use only accuracy to see how well the models do. As a model that predict 0 always would have a 0.91 Accuracy.  In this case I will look for the best F1 to tune the models parameters or hyperparameters. As F1 contains both precision and recall, so it tells if we are either missing to many positive values or overpredicting too many positive values. We want to find some balance as we dont want to give to many not pulsars to evaluate them as pulsar neither miss too many good candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1096d-e1aa-4cbe-8678-c9967fe86604",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to calculate evalution metrics. \n",
    "\n",
    "def calculate_precision(y_true, y_pred, pos_label_value=1.0):\n",
    "    '''\n",
    "    This function accepts the labels and the predictions, then\n",
    "    calculates precision for a binary classifier.\n",
    "    \n",
    "    Args\n",
    "        y_true: np.ndarray\n",
    "        y_pred: np.ndarray\n",
    "        \n",
    "        pos_label_value: (float) the number which represents the postiive\n",
    "        label in the y_true and y_pred arrays. Other numbers will be taken\n",
    "        to be the non-positive class for the binary classifier.\n",
    "    \n",
    "    Returns precision as a floating point number between 0.0 and 1.0\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    #precision is True Positive/(True Positives + False Positives)\n",
    "    TP = sum(y_true == pos_label_value)\n",
    "    #Check if y_pred == pos_label_value when y_true is not to get false positives\n",
    "    FP = sum(y_pred[(y_true!=pos_label_value)]==pos_label_value)\n",
    "    \n",
    "    \n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def calculate_recall(y_true, y_pred, pos_label_value=1.0):\n",
    "    '''\n",
    "    This function accepts the labels and the predictions, then\n",
    "    calculates recall for a binary classifier.\n",
    "    \n",
    "    Args\n",
    "        y_true: np.ndarray\n",
    "        y_pred: np.ndarray\n",
    "        \n",
    "        pos_label_value: (float) the number which represents the postiive\n",
    "        label in the y_true and y_pred arrays. Other numbers will be taken\n",
    "        to be the non-positive class for the binary classifier.\n",
    "    \n",
    "    Returns precision as a floating point number between 0.0 and 1.0\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    #Recall is TP/(TP+FN)\n",
    "    TP = sum(y_true == pos_label_value)\n",
    "    FN = sum(y_pred[(y_true == pos_label_value)]!=pos_label_value)\n",
    "\n",
    "                    \n",
    "    \n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def calculate_FPR(y_true, y_pred, pos_label_value=1.0):\n",
    "    #FPR is FP/N(data)\n",
    "    FP = sum(y_pred[y_true!=pos_label_value]==pos_label_value)\n",
    "    return(FP/len(y_true))\n",
    "\n",
    "def calculate_FNR(y_true,y_pred,pos_label_value = 1.0):\n",
    "    #FN/n(data)\n",
    "    FN = sum(y_pred[y_true==pos_label_value]!=pos_label_value)\n",
    "    return (FN/len(y_true))\n",
    "\n",
    "def cal_F1(y_true,y_pred,pos_value = 1.0):\n",
    "    return calculate_precision(y_true,y_pred)+calculate_recall(y_true,y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccce58-9190-488d-98c2-6bc77a47510e",
   "metadata": {},
   "source": [
    "I will fit 7 different classifiers to the data and compare its evaluation metrics. \n",
    "The classifiers will be:\n",
    "1. Logistic Regression\n",
    "2. K-Neiarest Neighbors\n",
    "3. Decision Tree \n",
    "4. Random Forest \n",
    "5. Extra Tree\n",
    "6. AdaBoost\n",
    "7. Linear Support Vector.\n",
    "\n",
    "\n",
    "The evaluation metrics to compare them will be:\n",
    "- The mean cross validation Accuracy.\n",
    "- Precision\n",
    "- Recall\n",
    "- F1, harmonic mean between precision and Recall.\n",
    "- The AUC score from the ROC curve\n",
    "- The False Positive Rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07553c7-f39d-458e-8e73-ea8826dd7b4e",
   "metadata": {},
   "source": [
    "For each classifier I will use the sklearn function of the classifier, fit it with the train data and do a parameter or hyperparameter search to get the \"best\" clasifier, I will the use such classifier to predict the test data, and when the model allows it to also get the predicted probability of the test data in order to get the ROC curve.  I will also show the confusion matrix of each classifier in order to see how many data points wher missclasified and how for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad230d-1c86-43cd-8d05-4d813a19ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame to store all the metrics from the diferent classifiers. \n",
    "#pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)\n",
    "comparison = pd.DataFrame(0,index = ['Mean_CV_Accuracy','Precision','Recall','F1','AUC','FPR'],\n",
    "                          columns = ['LR','KNN','DT','RF','ET','AB','SVL'], )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec1077-10f9-4556-baf6-cc2f2e9f83bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic regresion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ea00e-29e7-42e9-99b0-0ce3f7e27656",
   "metadata": {},
   "source": [
    "We start by fitting a simple logistic regression model with the train data specifing the class weight as balanced due to the imbalance on our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5970ec-e91d-4861-b7a9-b7467525a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We call and fit the logistic regresion model and display \n",
    "#the coeficients of each feature in the model. \n",
    "LogReg = LogisticRegression(class_weight='balanced',max_iter=100).fit(x_train,y_train)\n",
    "display(pd.DataFrame(LogReg.coef_,columns = x_test.columns.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed137c-9f4f-4877-90ee-d5501b8d2e86",
   "metadata": {},
   "source": [
    "As expected the most important feature in the logisitic regression, that is the one with the largest coeficient in the logistic regresion model was the Escess Kurtosis of the integrated profile. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff0d6a-d4c6-4ffd-ba34-6cbe4579571c",
   "metadata": {},
   "source": [
    "To evaluate how well the model did we calculate the precision, the recall, the ROC curve and the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739542-476f-4ce8-ac55-446f76b5f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the predicted target and the probability of the target to calculate the evaluation metrics\n",
    "#And see the ROC curve. \n",
    "ypp = LogReg.predict_proba(x_test)\n",
    "y_pred = LogReg.predict(x_test)\n",
    "\n",
    "# Calculate the evaluation metrics. \n",
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6771941-62e0-4979-bcc9-e62fbda530ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['LR']=(np.round(np.mean(cross_val_score(LogReg, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f46f1-5310-43fe-90f8-4e447475ed3a",
   "metadata": {},
   "source": [
    "The ROC curve shows the model works pretty well. However it seems like we are over predicting the True values. Lets see if further models get us a better F1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a7fcc-6225-43b4-8125-21261f168165",
   "metadata": {},
   "source": [
    "### KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de3694-3ed9-420f-b79f-cd7310f62a1c",
   "metadata": {},
   "source": [
    "Given that the number of features is not big and that we already scaled all the features we can try a KNN model to se how it performs against the rest. We will explore the number of neighbors to find the best knn classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8434a4-928f-4235-abe5-8e47336b6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighs = [] #K neigh clasifiers for diferents ns\n",
    "ns = np.arange(5,80,2)\n",
    "for n in ns :\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    neigh.fit(x_train,y_train)\n",
    "    neighs.append(neigh)\n",
    "\n",
    "    \n",
    "train_F1 = []\n",
    "test_F1 = []\n",
    "\n",
    "\n",
    "train_F1 = [cal_F1(y_train, clf.predict(x_train)) for clf in neighs]\n",
    "test_F1 = [cal_F1(y_test, clf.predict(x_test)) for clf in neighs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60462e-f035-4f6c-b069-ec882ca02cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Number of Neighbors (K)\")\n",
    "ax.set_ylabel(\"F1\")\n",
    "ax.set_title(\"F1 vs K for training and testing sets\")\n",
    "ax.plot(ns, train_F1, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ns, test_F1, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2855cc-3510-4705-903c-5fd353922ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia = np.where(test_F1 == max(test_F1))[0][0]\n",
    "print('The knn classifier with the best F1 has ' + str(ns[ia])+\" neighbors\")\n",
    "neigh = neighs[ia]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28e057-3c53-4ebb-995c-aa0993ab95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighcv = KNeighborsClassifier(n_neighbors=ns[ia])\n",
    "print('The accuracy of the KNN classifier with ' +str(ns[ia])+ ' neighbors is '+\\\n",
    "      str(np.round(np.mean(cross_val_score(neighcv, x_train, y_train, cv=3)),3)))\n",
    "\n",
    "y_pred = neigh.predict(x_test)\n",
    "ypp= neigh.predict_proba(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae7388-fc7e-491b-a920-be46596889e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451feaa3-c9b9-4963-bfbc-01bf253956dc",
   "metadata": {},
   "source": [
    "The KNN model had a better precision than the logistic regresion as the number of false positives it gives is small. However it has more false negatives, this means we miss more true positives of pulsar candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dfe26-c239-489a-a6e1-c7c7c1a0c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['KNN']=(np.round(np.mean(cross_val_score(neigh, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66469ca-de64-41ee-94e5-ce1d7aec2ea7",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eef516-3c29-461b-965b-09e52c3c1757",
   "metadata": {},
   "source": [
    "A simple decision tree and prune it for differents alphas, like we did in Module 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b42cbc-0565-49e7-a4d2-51bb83322990",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(max_depth=12, class_weight='balanced').fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee1e95-b3f4-4324-a9ec-2edd15c94aa8",
   "metadata": {},
   "source": [
    "We prune for a range of alphas and find the accuracy for each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ad76f-0750-4dab-a0e8-60cdbe759076",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DT.cost_complexity_pruning_path(x_train,y_train) #post pruning\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "clfs = [] # VECTOR CONTAINING CLASSIFIERS FOR DIFFERENT ALPHAS\n",
    "# TODO: iterate over ccp_alpha values \n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(class_weight='balanced',random_state=40, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "train_F1 = []\n",
    "test_F1 = []\n",
    "\n",
    "\n",
    "train_F1 = [cal_F1(y_train, clf.predict(x_train)) for clf in clfs]\n",
    "test_F1 = [cal_F1(y_test, clf.predict(x_test)) for clf in clfs]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"F1\")\n",
    "ax.set_title(\"F1 vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_F1, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_F1, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a0ad4-1a56-413f-87d4-8025cb888566",
   "metadata": {},
   "source": [
    "We choose the alpha where the test F1 is the largest.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b33cfc-935c-4cd2-b768-7414532b617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia = np.where(test_F1 == max(test_F1))[0][-1]\n",
    "BT = clfs[ia]\n",
    "\n",
    "print('The depth of the tree is ' + str(BT.get_depth()))\n",
    "print('The alpha used to prune the tree is ' + str(np.round(ccp_alphas[ia],4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984cea3-c4f2-4d2e-9296-51cacad0e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(class_weight='balanced',random_state=40, ccp_alpha=ccp_alphas[ia])\n",
    "print('The accuracy of the Decision Tree classifier is ' +\\\n",
    "      str(np.round(np.mean(cross_val_score(clf, x_train, y_train, cv=3)),3)))\n",
    "\n",
    "y_pred = BT.predict(x_test)\n",
    "ypp= BT.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c30fe7-ccd2-46bb-8949-55e2cce71c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b6b8a-6f6b-45e9-bd8d-110511d6d25a",
   "metadata": {},
   "source": [
    "With this very deep decision tree we improved the precision and the F1 values over the Logistic regression model results but the recall and AUC value decreased. In other words the False Positives decreased but the False Negatives increased, meaning with the decision tree we miss out on pulsar candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92da8f6-c5df-4b57-a578-4008e75160b4",
   "metadata": {},
   "source": [
    "Even with the prunning we got a pretty big the Decision Tree in order to get a better F1 metric than for logistic regresion the depth is 18. Such a deep tree usually overfits and we dint get the simplicity of a shaloow Decision Tree. \n",
    "A Decision tree ensemble method, with small trees could give better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1c5b-ad3c-4874-8ddf-4a22757e5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['DT']=(np.round(np.mean(cross_val_score(DT, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84c75a-687a-4f99-b2cd-7eaafee51fa6",
   "metadata": {},
   "source": [
    "### Ensemble trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfab8c6-e06d-4140-8ac9-3420aa3f3c79",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61390c62-0d12-4693-b41e-465e5cef64f5",
   "metadata": {},
   "source": [
    "A Random Forest Classifier like the one we saw in module 5,  with trees of max depth of 2, and using only 5% of the sample to train each tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2681c0d-c00a-41e7-b139-8d01626a702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(max_depth = 2, class_weight = 'balanced_subsample',\n",
    "                            max_samples = 0.05, random_state= 57)\n",
    "print('The accuracy of the random forest is ' +\\\n",
    "      str(np.round(np.mean(cross_val_score(RF, x_train, y_train, cv=3)),3)))\n",
    "RF.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dd629-9e87-4068-8d3a-da579d72e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(x_test)\n",
    "ypp =RF.predict_proba(x_test)\n",
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1740d0-c53c-40dc-83c4-927e564ff015",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['RF']=(np.round(np.mean(cross_val_score(RF, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ac8e8-834c-48e2-ba2f-80d4a3c8353d",
   "metadata": {},
   "source": [
    "#### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110955ee-88b0-4cd7-b5bb-66f98aa13843",
   "metadata": {},
   "source": [
    "An extra tree classifier or Extremely Randomized Trees, is similar to a random forest, with 2 differences, The basic extra tree algorithm uses all the train data in each decision tree, while the random forest bootstraps and uses samples, the second and more important difference is that the extra tree chooses the cut point to make the tree randomly, while the random forest chooses the optimum split. (https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/ ) becuase of it, the extra tree algorithm is faster than the random forest. Let's try it out and maybe even use more samples to create our classificator as it's faster. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html?highlight=ensemble+extra+tree+classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4380e7-f4fe-4834-937f-4a49e50818e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETC = ExtraTreesClassifier(n_estimators=100, class_weight = 'balanced',random_state=12)\n",
    "print('The accuracy of the Extra Tree Classifier is ' +\\\n",
    "      str(np.round(np.mean(cross_val_score(ETC, x_train, y_train, cv=3)),4)))\n",
    "\n",
    "ETC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90e47a-85b3-406b-ad2e-32aad7e6768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ETC.predict(x_test)\n",
    "ypp =ETC.predict_proba(x_test)\n",
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822465e1-c26a-4fe8-9b36-6d8d31cc1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['ET']=(np.round(np.mean(cross_val_score(ETC, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e904c-9f98-417f-bba7-f6ed97d47cf3",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b6d3e-dddd-4ca5-81ba-719f79bdcb33",
   "metadata": {},
   "source": [
    "Lets try a boosted ensemble method like adaboost we saw in module 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267d749-93e1-4599-9895-6b7d40e89bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = AdaBoostClassifier(n_estimators=100, random_state=25)\n",
    "print('The accuracy of the AdaBoost Classifier is ' +\\\n",
    "      str(np.round(np.mean(cross_val_score(AB, x_train, y_train, cv=3)),4)))\n",
    "\n",
    "AB.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba20f40-7192-455a-b744-66dbb52401a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = AB.predict(x_test)\n",
    "ypp =AB.predict_proba(x_test)\n",
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "#Calculate the roc curve from module 3.\n",
    "fpr,tpr,th = roc_curve(y_test,ypp[:,1])\n",
    "auc = roc_auc_score(y_test,ypp[:,1])\n",
    "plt.plot(fpr,tpr,\"-\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.text(0.65,0.25,'AUC= '+\"{:.3f}\".format(auc));\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8b91f-7b6b-4e0b-8149-d9c765d148e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['AB']=(np.round(np.mean(cross_val_score(AB, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  np.round(auc,4),np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e23a83-913b-4418-8dd8-414706e53a05",
   "metadata": {},
   "source": [
    "### Support vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79894506-5aa0-47e7-a02c-634f98fb04c6",
   "metadata": {},
   "source": [
    "From the pair plot looked like they could be a linear boundary so lets try a linear Support Vector Machine and do a grid search for the best parameter c. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088098d1-c209-4340-8eea-d6584cc423d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the variation in the C parameter. \n",
    "parameters = {'C':np.arange(1,12)}\n",
    "#We create a scorer so the grid search looks for the best F1.\n",
    "F1_scorer = make_scorer(cal_F1)\n",
    "lsvc = LinearSVC(loss ='squared_hinge')\n",
    "lss = GridSearchCV(lsvc, parameters, scoring=F1_scorer, cv = 3)\n",
    "grid = lss.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb04aba-6991-4c7f-a9f3-0808f7ba31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best parameters for the linear Suppport vector classifier is ' +\\\n",
    "str(grid.best_params_))\n",
    "print('The accuracy of the Linear SV Classifier is ' + str(np.round(grid.best_score_,4)))\n",
    "lsvcf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febf287-4a72-43aa-9c2a-30b6c00b85c4",
   "metadata": {},
   "source": [
    "Linear suport vector doesn't give you the probability of getting the target class, as the decision making is not by probabilities but by set boundaries, because of it there is no ROC curve or AUC value for this classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb5a7b-c1f0-49ce-b685-f908f5178f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsvcf.predict(x_test)\n",
    "pres = calculate_precision(y_test,y_pred)\n",
    "rec = calculate_recall (y_test,y_pred)\n",
    "F1 = cal_F1(y_test,y_pred)\n",
    "FPR = calculate_FPR(y_test,y_pred)\n",
    "print(\"Presicion is \" + str(np.round(pres,3)))\n",
    "print(\"Recall is \" + str(np.round(rec,3)))\n",
    "print(\"F1, the harmonic mean of precision and recall is \" + str(np.round(F1,3)))\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test,y_pred)\n",
    "#Confusion matrix plot from https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4a5c9-8d56-48b7-b547-e76e719fa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison['SVL']=(np.round(np.mean(cross_val_score(lsvcf, x_train, y_train, cv=3)),4),\n",
    "                  np.round(pres,4),np.round(rec,4),np.round(F1,4),\n",
    "                  None,np.round(FPR,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed45723-dfb3-46ec-befc-a3c896fa4ca4",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed338ef-ed83-44f8-82e3-e6a2d68b623c",
   "metadata": {},
   "source": [
    "This is the final table comparing the evalution metrics of each classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895f632-eeb7-4d33-98df-6bbc7c9af5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comparison.transpose().reset_index().rename(columns={\"index\":\"Class\"})\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e9b64-52ed-4fbe-9287-7b502cd1a42a",
   "metadata": {},
   "source": [
    "First we see the difference in values in the AUC for all but the SVL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec419ef-0cba-4512-a982-1df13ca952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = comp.sort_values(by = 'AUC')\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(comps.Class, comps.AUC,\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Classifier\")\n",
    "plt.ylabel(\"AUC values\")\n",
    "plt.title(\"AUC value for different classifiers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90a7b6-2b1f-4819-8836-0d26af929cdf",
   "metadata": {},
   "source": [
    "The Extra Tree classifier achieved the best AUC values, however the difference with all but the single Decision Tree is neglible as they are all around 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8614cf9-b80d-4532-a1dc-2a29bfdac234",
   "metadata": {},
   "source": [
    "We can visualize the rest of the evalution metrics in a parallel plot where the color of the line indicates the classifier and each vertical axis correspond to a evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74284d-db3c-4357-b0ce-53f16b5dd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(comp).transform_window(\n",
    "    index=\"count()\"\n",
    ").transform_fold(\n",
    "    ['Mean_CV_Accuracy', 'Precision', 'Recall','FPR','F1']\n",
    ").transform_joinaggregate(\n",
    "    min=\"min(value)\",\n",
    "    max=\"max(value)\",\n",
    "    groupby=[\"key\"]\n",
    ").transform_calculate(\n",
    "    norm_val=\"(datum.value - datum.min) / (datum.max - datum.min)\",\n",
    "    mid=\"(datum.min + datum.max) / 2\"\n",
    ").properties(width=600, height=300)\n",
    "\n",
    "lines = base.mark_line(opacity=0.6).encode(\n",
    "    x='key:N',\n",
    "    y=alt.Y('norm_val:Q', axis=None),\n",
    "    color=\"Class:N\",\n",
    "    detail=\"index:N\",\n",
    "    tooltip=[\"Mean_CV_Accuracy:N\", \"Precision:N\", \"Recall:N\", \"FPR:N\",\"F1:N\"]\n",
    ")\n",
    "\n",
    "rules = base.mark_rule(\n",
    "    color=\"#ccc\", tooltip=None\n",
    ").encode(\n",
    "    x=\"key:N\",\n",
    "    detail=\"count():Q\",\n",
    ")\n",
    "\n",
    "def ytick(yvalue, field):\n",
    "    scale = base.encode(x='key:N', y=alt.value(yvalue), text=f\"min({field}):Q\")\n",
    "    return alt.layer(\n",
    "        scale.mark_text(baseline=\"middle\", align=\"right\", dx=-5, tooltip=None),\n",
    "        scale.mark_tick(size=8, color=\"#ccc\", orient=\"horizontal\", tooltip=None)\n",
    "    )\n",
    "\n",
    "alt.layer(\n",
    "    lines, rules, ytick(0, \"max\"), ytick(150, \"mid\"), ytick(300, \"min\")\n",
    ").configure_axisX(\n",
    "    domain=False, labelAngle=0, tickColor=\"#ccc\", title=None\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2a983-01c4-4a4f-ab5a-a2bbb28ceb0a",
   "metadata": {},
   "source": [
    "The choice of classifier will depend on what its more important for us when predicting pulsar candidates, if the intended result is to miss as little pulsar candidates as posible then the Logistic Regression model is the best bet as it has the highest recall of all the classifiers. However if that was the metric we where after we would have to repeat the parameter and hyper parameter search for all the models using recall as the scoring. \n",
    "Here I focused on the best balance of results meaning the highest F1, so It wouldnt miss too many pulsar candidates but also wouldnt get too many false positives that would be a waist of resources looking at posible pulsars where they don't exist.  In the F1 race the biggest looser was the Logistic Regression classifier with a value of ~1.6. The decision tree and the Random Forest are around 1.7 and the other 4 models have really close F1 values around 1.8. \n",
    "\n",
    "If what we care the most is precision KNN and Linear Support Vector got the best results here with values around 0.96, but the ensemble methods Extra Tree and Ababoost allso had presicion results above 0.9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6540f35-7ec0-46ce-aaea-c936c7c1afda",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc4dc9-4db4-40f2-bea6-87c0e82ab14c",
   "metadata": {},
   "source": [
    "All the classificators based on different models did better than the trivial all negative or random 1/10 target. And depending on what we wanted we could choose a different model. \n",
    "I would say the 7 models worked pretty well. \n",
    "\n",
    "The main decisions to get to this results where in the data imputing, I would like to see what happende if instead of getting the missing values of the excess kurtosis from the other features I would have erased the column, or deleting the instances with missing values. However I choosed this as it could have been posible for the test data be missing the excess kurtosis and from the correlation matrix looked like a really important feature in the prediction of the target class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6620d8-a375-49a9-a3a2-94cc57ec440c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
